---
layout: post
title: The Legendre transform
comments: true
---
# The Legendre transform

## The Frenchel conjugate function

Let $f : X \rightarrow \mathbb R^n$. The function $f^\ast: \mathbb R^n \rightarrow \mathbb R$ defined as:

$$f^\ast(y) = \sup_{x \in X} (y^T x - f(x))$$

is called the conjugate (or Frenchel conjugate) of $f$. This definition can be used even if $f$ is not convex. However, $f^\ast(y)$ is always convex (even if $f$ isn't), because it is the supremum of a family of convex (affine) functions in $y$. The definition of $f^\ast(y)$ implies that,

$$f^\ast(y) + f(x) \ge y^T x$$

called Fenchel's inequality. A point $x^\ast\in X$ is a maximizer of $y^T x - f(x)$ if and only if we have equality,

$$f^\ast(y) + f(x^\ast) = y^T x^\ast$$

## The Legendre transform

If $f$ is **differentiable and convex**, $f^\ast$ is also called the *Legendre transform* of $f$. A maximizer $x^\ast$ of $y^T x - f(x)$ must satisfy $\nabla f(x^\ast) = y$. Conversely, if $y = \nabla f(x^\ast)$ for some $x^\ast$, then $x^\ast$ is a maximizer of $y^T x - f(x)$ because $y^T x - f(x)$ is concave in $x$ (see [Boyd2004] §3.1.3). Therefore,

$$
f^\ast(y) = y^T x - f(x) \quad \text{whenever} \quad y = \nabla f(x)
$$

If for a given $y$ we solve $y = \nabla f(x)$ for $x$, this equation determines the value of $f^\ast(y)$. 

Since $f$ is convex, the gradient function $\nabla f$ is invertible. Setting $x = (\nabla f)^{-1}(y)$ we can evaluate the derivative of $f^\ast$ as follows,

$$\begin{aligned}
\frac{\partial f^\ast}{\partial y} 
&= \frac{\partial}{\partial y} (y^T x - f(x)) \\
&= x + y^T \frac{\partial x}{\partial y} - \frac{\partial f}{\partial x}  \frac{\partial x}{\partial y} 
\end{aligned}$$

Since $\nabla f(x) = y$, the two last terms cancel and we are left with

$$\nabla f^\ast(y) = x = (\nabla f)^{-1}(y)$$

Therefore the functions $\nabla f(x)$ and $\nabla f^\ast (y)$ are inverses of each other. To highlight the symmetry we can write:

$$\nabla f(x) = y \quad \iff \quad \nabla f^\ast(y) = x$$

Now let us compute the dual of $f^\ast(y)$,

$$f^{\ast\ast}(x) = \sup_y (x^T y - f^\ast (y))$$

Pick $y$ such that $\nabla f^\ast(y) = x$. Then

$$f^{\ast\ast}(x) = x^T y - f^\ast(y)$$

But $\nabla f^\ast(y) = x$ also implies that $\nabla f(x) = y$, which in turn implies $f^\ast(y) = y^T x - f(x)$. Substituting we obtain the Legendre duality

$$f^{\ast\ast}(x) = f(x)$$

Thus the conjugate of the conjugate of a differentiable function is the function itself.

### Representation as pointwise supremum of affine functions

Notice that $y^T x - f^*(y)$ is an affine function of $x$. Therefore, any differentiable convex function can be represented as a supremum of affine functions:

$$f(x) = \sup_y (y^T x - f^*(y))$$

### Convex constrains

Let $f:X \rightarrow \mathbb R$ be a convex differentiable function. Let $g: X' \rightarrow \mathbb R$ be a restriction of $f$ to a convex subset $\hat X \subset X$. Then

$$\begin{aligned}
g^*(y) &= \sup_{x \in X'}(y^T x - g(x)) \\
    &= \sup_{x \in X'}(y^T x - f(x)) \\
    &\le \sup_{x \in X}(y^T x - f(x)) = f^*(y)
\end{aligned}$$

Since $g(x) = f(x)$ for all $x \in X'$, Legendre duality implies that

$$g(x) = \sup_y(y^T x - g^*(y)) = \sup_y (y^T x - f^*(y))$$

Although $g^*(y)$ might be different from $f^*(y)$, it turns out that both can be used to represent $g(x)$ as a supremum of affine functions.

### Examples

#### Affine transformations

The conjugate of $g(x) = a f(x) + b$ where $a > 0$ is

$$\begin{aligned}
g^*(y) &= \sup_x (yx - a f(x) - b) \\
    &= a \sup_x \left(\frac{yx}{a} - f(x) \right) - b \\
    &= a f^*(y/a) - b
\end{aligned}$$

If $g(x) = f(Ax+b)$, with $A$ an invertible square matrix, then,

$$\begin{aligned}
g^\ast(y) &= \sup_x (y^T x - f(Ax+b)) \\
    &= \sup_x (y^T A^{-1}(x - b) - f(x)) \\
    &= \sup_x ((A^{-T}y)^T x - f(x)) - y^T A^{-1} b \\
    &= f^*(A^{-T}y) - y^T A^{-1} b
\end{aligned}$$

where $A^{-T}$ denotes the inverse transpose. The domain of $g^*$ is the set of $y$ such that $A^{-T}y$ is in the domain of $f^*$.

#### Addition of an affine term

The cojungate of $g(x) = f(x) + c^T x + d$ is,

$$\begin{aligned}
g^\ast(y) &= \sup_x (y^T x - f(x) - c^T x - d) \\
    &= \sup_x (y^T x - f(x) - c^T x - d) \\
    &= \sup_x ((y - c)^T x - f(x)) - d \\
    &= f^*(y - c) - d
\end{aligned}$$

The domain of $g^*$ is the set of $y$ such that $y - c$ is in the domain of $f^*$.

#### Relation to Legendre dual function

There is an important connection to the Legendre dual function. Consider a convex optimization problem with linear constrains of the form:

$$
\text{minimize } f_0(x)
\text{ subject to } A x \le b
\text{ and } Cx = d
$$

where $f_0$ is convex, $A,C$ are matrices and $\le$ denotes component-wise inequality. The Lagrangian of this program is

$$L(x,\lambda,\nu) = f_0(x) + \lambda^T(Ax - b) + \nu^T(Cx - d)$$

and the Lagrangian dual function, $g(\lambda,\nu) = \inf_x L(x,\lambda,\nu)$. Note that,

$$\begin{aligned}
g(\lambda,\nu) &= \inf_x (f_0(x) + \lambda^T (Ax-b) + \nu^T(Cx-d)) \\
&= -\sup_x (-(\lambda^T A + \nu^T C)x - f_0(x)) - \lambda^T b - \nu^T d \\
&= -f_0^*(-A^T\lambda - C^T\nu) - b^T\lambda - d^T\nu
\end{aligned}$$

where $f_0^*$ is the conjugate of $f_0$.

#### Order-inversion

The conjugate operation is decreasing in the following sense. 
If $f,g$ have the same domain $X$ and $f(x) \le g(x)$ for all $x \in X$, then 

$$f^\ast(y) = \sup_{x \in X} (y^Tx - f(x)) \ge \sup_{x \in X} (y^Tx - g(x)) = g^\ast(y)$$

#### Sum of independent functions

If $f(u,v) = f_1(u) + f_2(v)$, where $f_1,f_2$ are convex, then

$$\begin{aligned}
f^\ast(w,z) &= \sup_{u,v}(wu + zv - f_1(u) - f_2(v)) \\
    &= \sup_{u}(wu - f_1(u)) + \sup_{v}(zv - f_2(v)) \\
    &= f_1^\ast(w) + f_2^\ast(z)
\end{aligned}$$

This result depends on the fact that $f(u,v)$ is the sum of two functions that depend on different variables.
In general, the Legendre transform of a sum of functions that depend on a common set of variables is not equal to the sum of the Legendre transforms of the functions.

#### Reciprocal

Take $f(x) = 1/x$ with domain $x > 0$. For $y>0$, $yx - 1/x$ grows without bounds as $x \rightarrow \infty$. If $y=0$ then $yx - 1/x = -1/x < 0$ has the supremum $0$ as $x \rightarrow 0$. For $y < 0$, $yx - 1/x$ has a critical point at $x^* = \sqrt{-1/y}$ (the negative square root is discarded because $x > 0$). Substituting,

$$f^*(y) = yx^* - 1/x^* = -2\sqrt{-y}$$

## References

**[Boyd2004]** Boyd, Stephen P., and Lieven Vandenberghe. Convex Optimization. Cambridge, UK ; New York: Cambridge University Press, 2004. Specially §3.3.



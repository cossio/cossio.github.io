---
layout: post
title: The Legendre transform
comments: true
---
# The Legendre transform

## The Frenchel conjugate function

Let $f : X \rightarrow \mathbb R^n$. The function $f^\ast: \mathbb R^n \rightarrow \mathbb R$ defined as:

$$f^\ast(y) = \sup_{x \in X} (y^T x - f(x))$$

is called the conjugate (or Frenchel conjugate) of $f$. This definition can be used even if $f$ is not convex. However, $f^\ast(y)$ is always convex (even if $f$ isn't), because it is the supremum of a family of convex (affine) functions in $y$. The definition of $f^\ast(y)$ implies that,

$$f^\ast(y) + f(x) \ge y^T x$$

called **Fenchel's inequality**. A point $x^\ast\in X$ is a maximizer of $y^T x - f(x)$ if and only if we have equality,

$$f^\ast(y) + f(x^\ast) = y^T x^\ast$$

## The Legendre transform

If $f$ is **differentiable and convex**, $f^\ast$ is also called the *Legendre transform* of $f$. A maximizer $x^\ast$ of $y^T x - f(x)$ must satisfy $\nabla f(x^\ast) = y$. Conversely, if $y = \nabla f(x^\ast)$ for some $x^\ast$, then $x^\ast$ is a maximizer of $y^T x - f(x)$ because $y^T x - f(x)$ is concave in $x$ (see [Boyd2004] §3.1.3). Therefore,

$$
f^\ast(y) = y^T x - f(x) \quad \text{whenever} \quad y = \nabla f(x)
$$

If for a given $y$ we solve $y = \nabla f(x)$ for $x$, this equation determines the value of $f^\ast(y)$. 

Since $f$ is convex, the gradient function $\nabla f$ is invertible. Setting $x = (\nabla f)^{-1}(y)$ we can evaluate the derivative of $f^\ast$ as follows,

$$\begin{aligned}
\frac{\partial f^\ast}{\partial y} 
&= \frac{\partial}{\partial y} (y^T x - f(x)) \\
&= x + y^T \frac{\partial x}{\partial y} - \frac{\partial f}{\partial x}  \frac{\partial x}{\partial y} 
\end{aligned}$$

Since $\nabla f(x) = y$, the two last terms cancel and we are left with

$$\nabla f^\ast(y) = x = (\nabla f)^{-1}(y)$$

Therefore the functions $\nabla f(x)$ and $\nabla f^\ast (y)$ are inverses of each other. To highlight the symmetry we can write:

$$\nabla f(x) = y \quad \iff \quad \nabla f^\ast(y) = x$$

Now let us compute the dual of $f^\ast(y)$,

$$f^{\ast\ast}(x) = \sup_y (x^T y - f^\ast (y))$$

Pick $y$ such that $\nabla f^\ast(y) = x$. Then

$$f^{\ast\ast}(x) = x^T y - f^\ast(y)$$

But $\nabla f^\ast(y) = x$ also implies that $\nabla f(x) = y$, which in turn implies $f^\ast(y) = y^T x - f(x)$. Substituting we obtain the Legendre duality

$$f^{\ast\ast}(x) = f(x)$$

Thus the conjugate of the conjugate of a differentiable function is the function itself.

### Representation as pointwise supremum of affine functions

Notice that $y^T x - f^*(y)$ is an affine function of $x$. Therefore, any differentiable convex function can be represented as a supremum of affine functions:

$$f(x) = \sup_y (y^T x - f^*(y))$$

### Convex constrains

Let $f:X \rightarrow \mathbb R$ be a convex differentiable function. Let $g: X' \rightarrow \mathbb R$ be a restriction of $f$ to a convex subset $\hat X \subset X$. Then

$$\begin{aligned}
g^*(y) &= \sup_{x \in X'}(y^T x - g(x)) \\
    &= \sup_{x \in X'}(y^T x - f(x)) \\
    &\le \sup_{x \in X}(y^T x - f(x)) = f^*(y)
\end{aligned}$$

Since $g(x) = f(x)$ for all $x \in X'$, Legendre duality implies that

$$g(x) = \sup_y(y^T x - g^*(y)) = \sup_y (y^T x - f^*(y))$$

Although $g^*(y)$ might be different from $f^*(y)$, it turns out that both can be used to represent $g(x)$ as a supremum of affine functions.

### Examples

#### Affine transformations

The conjugate of $g(x) = a f(x) + b$ where $a > 0$ is

$$\begin{aligned}
g^*(y) &= \sup_x (yx - a f(x) - b) \\
    &= a \sup_x \left(\frac{yx}{a} - f(x) \right) - b \\
    &= a f^*(y/a) - b
\end{aligned}$$

If $g(x) = f(Ax+b)$, with $A$ an invertible square matrix, then,

$$\begin{aligned}
g^\ast(y) &= \sup_x (y^T x - f(Ax+b)) \\
    &= \sup_x (y^T A^{-1}(x - b) - f(x)) \\
    &= \sup_x ((A^{-T}y)^T x - f(x)) - y^T A^{-1} b \\
    &= f^*(A^{-T}y) - y^T A^{-1} b
\end{aligned}$$

where $A^{-T}$ denotes the inverse transpose. The domain of $g^*$ is the set of $y$ such that $A^{-T}y$ is in the domain of $f^*$.

#### Addition of an affine term

The cojungate of $g(x) = f(x) + c^T x + d$ is,

$$\begin{aligned}
g^\ast(y) &= \sup_x (y^T x - f(x) - c^T x - d) \\
    &= \sup_x (y^T x - f(x) - c^T x - d) \\
    &= \sup_x ((y - c)^T x - f(x)) - d \\
    &= f^*(y - c) - d
\end{aligned}$$

The domain of $g^*$ is the set of $y$ such that $y - c$ is in the domain of $f^*$.

#### Relation to Legendre dual function

There is an important connection to the Legendre dual function. Consider a convex optimization problem with linear constrains of the form:

$$
\text{minimize } f_0(x)
\text{ subject to } A x \le b
\text{ and } Cx = d
$$

where $f_0$ is convex, $A,C$ are matrices and $\le$ denotes component-wise inequality. The Lagrangian of this program is

$$L(x,\lambda,\nu) = f_0(x) + \lambda^T(Ax - b) + \nu^T(Cx - d)$$

and the Lagrangian dual function, $g(\lambda,\nu) = \inf_x L(x,\lambda,\nu)$. Note that,

$$\begin{aligned}
g(\lambda,\nu) &= \inf_x (f_0(x) + \lambda^T (Ax-b) + \nu^T(Cx-d)) \\
&= -\sup_x (-(\lambda^T A + \nu^T C)x - f_0(x)) - \lambda^T b - \nu^T d \\
&= -f_0^*(-A^T\lambda - C^T\nu) - b^T\lambda - d^T\nu
\end{aligned}$$

where $f_0^*$ is the conjugate of $f_0$.

#### Order-inversion

The conjugate operation is decreasing in the following sense. 
If $f,g$ have the same domain $X$ and $f(x) \le g(x)$ for all $x \in X$, then 

$$f^\ast(y) = \sup_{x \in X} (y^Tx - f(x)) \ge \sup_{x \in X} (y^Tx - g(x)) = g^\ast(y)$$

#### Sum of independent functions

If $f(u,v) = f_1(u) + f_2(v)$, where $f_1,f_2$ are convex, then

$$\begin{aligned}
f^\ast(w,z) &= \sup_{u,v}(wu + zv - f_1(u) - f_2(v)) \\
    &= \sup_{u}(wu - f_1(u)) + \sup_{v}(zv - f_2(v)) \\
    &= f_1^\ast(w) + f_2^\ast(z)
\end{aligned}$$

This result depends on the fact that $f(u,v)$ is the sum of two functions that depend on different variables.
In general, the Legendre transform of a sum of functions that depend on a common set of variables is not equal to the sum of the Legendre transforms of the functions.

#### Reciprocal

Take $f(x) = 1/x$ with domain $x > 0$. For $y>0$, $yx - 1/x$ grows without bounds as $x \rightarrow \infty$. If $y=0$ then $yx - 1/x = -1/x < 0$ has the supremum $0$ as $x \rightarrow 0$. For $y < 0$, $yx - 1/x$ has a critical point at $x^* = \sqrt{-1/y}$ (the negative square root is discarded because $x > 0$). Substituting,

$$f^*(y) = yx^* - 1/x^* = -2\sqrt{-y}$$

## Concave conjugate

Let $f(x)$ be a concave differentiable function. Let $g^*(\xi)$ be the Legendre transform of the convex function $g(x) = -f(x)$. Then,

$$g(x) = \sup_\xi (\xi^T x - g^*(\xi)) = \sup_\xi (\xi^T x + f^*(-\xi)) = \sup_\xi (-\xi^T x + f^*(\xi))$$

where

$$f^*(\xi) = -g^*(-\xi) = -\sup_x (-\xi^T x - g(x))$$

is concave. Hence,

$$f(x) = \inf_\xi (\xi^T x - f^*(\xi))$$
$$f^*(\xi) = \inf_x (\xi^T x - f(x))$$

These relations are the concave version of the Legendre transform relations derived above for convex functions. In particular every concave differentiable function can be represented as an infimum of affine functions.


### Saddle-functions

Let $f(x,y)$ be a saddle-function (convex in $x$ and concave in $y$). Since $f(x,y)$ is convex in $x$,

$$f(x,y)=\sup_{\xi}(\xi^T x-\hat f(\xi,y))$$

where

$$\hat f(\xi,y) = \sup_x (\xi^T x - f(x,y))$$

is the Legendre transform of $f(x,y)$ w.r.t. $x$. The function $\hat f(\xi,y)$ is convex in $y$, because it is the supremum of a family of convex functions in $y$ (see Boyd 2004, §3.2). Therefore,

$$\hat f(\xi,y) = \sup_\eta (\eta^T y - \hat f(\xi,\eta))$$

where $\hat f(\xi,\eta)$ is the the Legendre transform of $\hat f(\xi,y)$ w.r.t. $y$,

$$\hat f(\xi,\eta) = \sup_y (\eta^T y - \hat f(\xi,y))$$

By substitution we obtain the equations

$$\hat f(\xi,\eta) = \sup_y \inf_x (\eta^T y - \xi^T x + f(x,y))$$
$$f(x,y) = \sup_\xi \inf_\eta (\xi^T x - \eta^T y + \hat f(\xi,\eta))$$

Notice that $\hat f(\xi,\eta)$ is convex in $\eta$ because it is a Legendre transform (of $\hat f(\xi,y)$). However it need not be convex nor concave in $\xi$!

We could have started using the "concave version of the Legendre transform" derived above. We have:

$$f(x,y) = \inf_\eta (\eta^T y - \bar f(x,\eta))$$

where

$$\bar f(x,\eta) = \inf_y (\eta^T y - f(x,y))$$

is concave in $x$, because it is the infimum of a family of concave functions. Next,

$$\bar f(x,\eta) = \inf_\xi (\xi^T x - \bar f(\xi,\eta))$$

where

$$\bar f(\xi,\eta) = \inf_x (\xi^T x - \bar f(x,\eta))$$

Therefore,

$$f(x,y) = \inf_\eta \sup_\xi (\eta^T y - \xi^T x + \bar f(\xi,\eta))$$

$$\bar f(\xi,\eta) = \inf_x \sup_y (\xi^T x - \eta^T y + f(x,y))$$

Notice that the order of optimizations has changed. Moreover, $\bar f(\xi,\eta)$ is now a convex function of $\xi$, although it need not be convex nor concave in $\eta$.


## References

**[Boyd2004]** Boyd, Stephen P., and Lieven Vandenberghe. Convex Optimization. Cambridge, UK ; New York: Cambridge University Press, 2004. Specially §3.3.



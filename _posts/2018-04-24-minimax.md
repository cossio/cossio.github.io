---
layout: post
title: The minimax inequality and saddle-points
comments: true
---
# The minimax inequality and saddle-points

Let $f:X\times Y\rightarrow\mathbb R$ be a function. The *minimax* inequality states that:

$$\sup_{y\in Y}\inf_{x\in X}f(x,y) \le \inf_{x\in X}\sup_{y\in Y}f(x,y)$$

The proof is simple. For all $x'\in X,y'\in Y$,

$$\inf_{x\in X}f(x,y')\le f(x',y')\le\sup_{y\in Y}f(x',y)$$

by the definitions of $\inf$ and $\sup$. Since these inequalities hold for all $x'\in X,y'\in Y$, they still hold if we take the $\inf$ w.r.t. $x'$ and the $\sup$ w.r.t. to $y'$. Dropping the primes from the dummy variables $x',y'$ we obtain the minimax inequality.

Note that this proof, depends critically on the assumption that the domain of $f$ is a Cartesian product, $\mathrm{dom} \, f = X \times Y$. If this assumption is relaxed, the minimax inequality need not hold. We investigate this further in a second note.

## Saddle-points

A point $x_0\in X,y_0 \in Y$ satisfying:

$$f\left(x_{0},y\right)\le f\left(x_{0},y_{0}\right)\le f\left(x,y_{0}\right)$$

for all $x\in X,y\in Y$ is called a **saddle point** of $f$. If $x_0,y_0$ is a saddle-point, then

$$
\inf_{x\in X}\sup_{y\in Y}f\left(x,y\right)\le\inf_{x\in X}f\left(x,y_{0}\right)=f\left(x_{0},y_{0}\right)
$$

$$
\sup_{y\in Y}\inf_{x\in X}f\left(x,y\right)\ge\sup_{y\in Y}f\left(x_{0},y\right)=f\left(x_{0},y_{0}\right)
$$

Together with the minimax inequality, this implies the *minimax equality*,

$$\sup_{x\in X}\inf_{y \in Y} f(x,y) = \inf_{y\in Y}\sup_{x \in X} f(x,y)$$

Moreover, notice that $f(x_0,y_0)$ has been sandwiched and therefore equals the result of both extremizations. This holds for *all* saddle points, because the extremizations made no reference to a particular saddle-point. Therefore all saddle-points produce the same value of $f$.

Thus, the existence of a saddle-point guarantees that the minimax inequality becomes an equality. What about the converse? The minimax equality implies existence of saddle-points? The answer is Yes, as we will see, but only if a $f$ satisfies a technical condition.

Suppose that there exist $x_0\in X, y_0 \in Y$ such that $x_0$ is a maximizer of $\inf_{y \in Y} f(x,y)$ and $y_0$ a minimizer of $\sup_{x \in X} f(x,y)$. For example, if $f(x,y)$ is continuous and $X,Y$ compact, then $x_0,y_0$ are guaranteed to exist. Then,

$$\begin{aligned}
  \sup_{x \in X} \inf_{y \in Y} f(x,y) = \inf_{y \in Y} f(x_0, y) &\le f(x_0, y_0) \\
  \inf_{y \in Y} \sup_{x \in X} f(x,y) = \sup_{x \in X} f(x, y_0) &\ge f(x_0, y_0) \\
\end{aligned}$$

Hence, if the minimax equality holds, and $f$ satisfies a regularity condition, $x_0,y_0$ is a saddle-point.

Therefore the existence of a saddle-point is a necessary and sufficient condition for the minimax equality to hold, provided $f(x,y)$ is "smooth" enough to guarantee the existence of the extremizer values $x_0,y_0$.

Suppose that $x_0,y_0$ is a saddle-point of a differentiable function $f(x,y)$. Then $x_0$ is a minimizer of $f(x,y_0)$ with respect to $x$, and $y_0$ is a maximizer of $f(x_0,y)$ with respect to $y$. Moreover suppose that $x_0$ is in the interior of $X$ (_i.e._, that a ball centered at $x_0$ is contained in $X$), and that $y_0$ is in the interior of $Y$. Then

$$\left.\frac{\partial f}{\partial x}\right|_{(x_0,y_0)} = \left.\frac{\partial f}{\partial y}\right|_{(x_0,y_0)} = 0$$

Therefore interior saddle-points of differentiable functions are critical points.

## The set of saddle-points is a "rectangle"

Let $(x_1^\ast,y_1^\ast)$ and $(x_2^\ast,y_2^\ast)$ be two saddle-points of an arbitrary function $f: X\times Y\rightarrow\mathbb R$. Then

$$f(x_1^\ast,y_1^\ast) = f(x_2^\ast,y_2^\ast)=f^\ast$$

By definition,

$$f(x_1^\ast,y) \le  f^\ast \le f(x,y_1^\ast)$$

$$f(x_2^\ast,y) \le f^\ast \le f(x,y_2^\ast)$$

for all $x\in X,y\in Y$. In particular,

$$f(x_1^\ast,y_2^\ast) \le f^\ast \le f(x_2^\ast,y_1^\ast)$$

$$f(x_2^\ast,y_1^\ast) \le f^\ast \le f(x_1^\ast,y_2^\ast)$$

which implies $f(x_1^\ast,y_2^\ast) = f(x_2^\ast,y_1^\ast) = f^\ast$, and

$$f(x_1^\ast,y) \le f(x_1^\ast,y_2^\ast) \le f(x,y_2^\ast)$$

$$f(x_2^\ast,y) \le f(x_2^\ast,y_1^\ast) \le f(x,y_1^\ast)$$

for all $x\in X,y\in Y$. Therefore $(x_1^\ast,y_2^\ast)$ and $(x_2^\ast,y_1^\ast)$ are also saddle-points.

Let $\mathcal S$ denote the set of all saddle-points. The above argument shows that $\mathcal S = \mathcal X\times \mathcal Y$, for some $\mathcal X\subset X,\mathcal Y\subset Y$. More precisely, $\mathcal X$ is the set of all $x$-coordinates of all saddle-points and $\mathcal Y$ is the set of all $y$-coordinates of all saddle-poins.

Thus, the set of all saddle-points is a "rectangle".

## Saddle functions

The function $f:X\times Y \rightarrow \mathbb R$ is a *saddle-function* if $X,Y$ are convex and $f(x,y)$ is convex in $x$ (for fixed $y$) and concave in $y$ (for fixed $x$). More generally, the variables of a saddle-function can be partitioned such that fixing one subset of the variables makes the function convex, but fixing the other subset makes the function concave.

All critical points of a differentiable saddle-function are saddle-points. Indeed, suppose that $\nabla f (x^\ast, y^\ast) = 0$. Then $x^\ast$ must be a minimizer of $f(x,y^\ast)$ because $f(x,y_0)$ is a convex function of $x$. Similarly, $y^\ast$ is a maximizer of $f(x^\ast,y)$. Therefore

$$f(x^\ast,y) \le f(x^\ast,y^\ast) \le f(x,y^\ast)$$

holds for all $x\in X,y\in Y$, and $(x^\ast,y^\ast)$ is a saddle-point. Conversely, all interior saddle-points are critical (as shown above, this holds for any function, not just saddle-functions). In addition, all saddle-points $(x^\ast,y^\ast)$ satisfy the minimax equality:

$$\sup_x \inf_y f(x,y) = \inf_y \sup_x f(x,y) = f(x^\ast,y^\ast)$$

Therefore all saddle-points produce the same value of $f$. Conversely, if the point $(x^\ast,y^\ast)$ solves any one of these optimizations, and is an interior point, it must be a critical point and hence a saddle-point.

### Minimization of convex variables and maximization of concave variables can be carried out in any order

More generally, suppose I extremize all variables $x,y$ in some order. For example

$$\inf_{x_1} \sup_{y_2} \sup_{y_1} \inf_{x_4} \dots f(x,y)$$

We suppose that all the variables are extremized, so that the result is a single number (can be infinity). We restrict our attention to the case where convex variables are minimized and concave variables are maximized, because if a convex variable is maximized (or a concave variable minimized) the result can be unbounded. The question is whether the order in which these extremizations are carried out is important. For example, does equality hold in the following equation?

$$\inf_{x_2} \sup_{y_4} \inf_{x_1} \dots f(x,y) = \inf_{x_5} \inf_{x_4} \sup_{y_1} \dots f(x,y) \qquad (?)$$

Provided that the saddle-function $f$ is differentiable and that there exists an extremizer point $(x^\ast,y^\ast)$ where $f(x^\ast,y^\ast)$ assumes the optimal value of the extremization, it follows that $(x^\ast,y^\ast)$ is critical and hence a saddle-point. Therefore the value $f(x^\ast, y^\ast)$ equals the common value of all saddle-points. Thus the order of optimizations is not important.

#### Partial optimizations

Suppose now that the variables $x,y$ are partitioned into groups $x = (x_1,x_2), y = (y_1,y_2)$. The domains of the variables $x_1,y_1$ may depend on the values of $x_2,y_2$. Therefore we define,

$$X_1(x_2) = \{x_1 : (x_1,x_2) \in X\}$$
$$Y_1(y_2) = \{y_1 : (y_1,y_2) \in Y\}$$

For any $x_2,y_2$, the sets $X_1(x_2)$ and $Y_1(y_2)$ are convex. Indeed, a line connecting $(x_1,x_2)$ to $(x_1', x_2)$ is guaranteed to exist in $X$ because it is convex. All the points in this line have the same value of $x_2$ and therefore also belong to $X_1(x_2)$. Therefore $X_1(x_2)$ is convex. A similar argument shows that $Y_1(y_2)$ is convex too.

The restricted function will be denoted by $f_{x_2,y_2}: X_1(x_2)\times Y_1(y_2) \rightarrow \mathbb R$. By definition

$$f_{x_2,y_2}(x_1,y_1) = f(x,y)$$

whenever

$$(x_1, x_2) \in X, (y_1, y_2) \in Y$$

For each fixed $x_2,y_2$, the function $f_{x_2,y_2}$ is a saddle-function (convex in $x_2$ and concave in $y_2$) of the variables $x_1,y_1$ (convex in $x_1$ and concave in $y_1$).

Consider a partial optimization where only a subset of the variables is extremized. We optimize over $x_2,y_2$ keeping $x_1,y_1$ fixed. The result is a function $g(x_1,y_1)$ of the "free" variables $x_1,y_1$:

$$g(x_1,y_1) = \inf_{x_2} \sup_{y_2} f(x,y) = \sup_{y_2} \inf_{x_2} f(x,y)$$

By our previous results applied to $f_{x_2,y_2}(x_1,y_1)$, any order of the optimizations will produce the same value of $g(x_1,y_1)$, provided $f$ is sufficiently smooth that extremizer arguments exist in each case.

Fix $y_1$. Then $\sup_{y_2} f(x,y)$ is convex in $x_1$, because it is the supremum of a family of convex functions (Boyd2004 §3.2.3). But it need not be concave in $y_2$. Therefore $g(x_1,y_1)$ is not a saddle-function in general.

## References

1. J. von Neumann and O. Morgenstern, Theory of Games and Economic Behavior, 3rd ed. (John Wiley & Sons Inc, 1953). Specially §13.4.3 and §13.5.1.
2. [Wikipedia page on the minimax theorem](https://en.wikipedia.org/wiki/Minimax_theorem)
3. Boyd, Stephen P., and Lieven Vandenberghe. Convex Optimization. Cambridge, UK ; New York: Cambridge University Press, 2004.
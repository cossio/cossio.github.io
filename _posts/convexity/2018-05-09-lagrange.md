---
layout: post
title: Lagrangian duality
comments: true
---
# Lagrangian duality

## Primal problem

Let $f,g_i,h_j$ be functions $X\rightarrow\mathbb R$, where $X\subset\mathbb R^n$. Consider the constrained minimization problem

$$ f^\ast = \min_{x\in\Omega} f(x) $$

where $\Omega\subset X$ is the subset of points $x\in X$ satisfying the constrains

$$g_i(x)\le0,\quad i=1,\dots,p$$

$$h_j(x)=0,  \quad i=1,\dots,q$$

We assume that $f,g_i,h_j$ are convex functions in $x$ and that $X$ is a convex set. Then $\Omega$ is also convex.

## Lagrangian and Lagrange dual function

Define the Lagrangian $L$ and the Lagrange dual function $G$,

$$L(x,\lambda,\nu) = f(x) + \sum_i\lambda_i g_i(x) + \sum_j\nu_jh_j(x)$$

$$G(\lambda,\nu) = \min_{x\in X} L(x,\lambda,\nu)$$

$G$ is a concave function of $\lambda,\nu$, because it is the minimum of a family of affine functions in these variables. If $\lambda_i\ge0$ for all $i$ and $x\in\Omega$, then $L(x,\lambda,\nu)\le f(x)$, because $g_i(x)\le0$ for all $i$ and $h_j(x)=0$ for all $j$. Since this inequality holds for all $x\in\Omega$, it follows that

$$G(\lambda,\nu) \le \min_{x\in\Omega} f(x)$$

for all $\lambda\ge0,\nu$. We assume that strong duality holds. This is where the convexity assumptions made above are critical, since then mild regularity conditions (*e.g.* Slater constraint qualifications) are enough to guarantee strong duality. Then,

$$\max_{\lambda\ge0}\max_\nu G(\lambda,\nu) = \min_{x\in\Omega}f(x)$$

Substituting the definition of $G$,

$$\min_{x\in\Omega}f(x) = \max_{\lambda\ge0}\max_\nu\min_{x\in X}L(x,\lambda,\nu)$$

## The saddle-points of $L$

For fixed $x$, we can optimize $L$ w.r.t. $\lambda\ge0,\nu$. It is easy to see that

$$\max_{\lambda \ge 0}\max_\nu L(x,\lambda,\nu) =
\begin{cases}
  f(x)      & x\in\Omega \\
  \infty    & x\notin\Omega
\end{cases}$$

Therefore, minimizing w.r.t to $x$ yields a feasible point $x^\ast\in\Omega$. It follows that:

$$\min_{x\in X}\max_{\lambda\ge0}\max_\nu L(x,\lambda,\nu) = \min_{x\in\Omega}f(x)$$

We note in passing that this equality does not require convexity of $\Omega,f$ and it does not depend on constrain qualifications. Comparing with the equality derived above (which does require convexity and some constraint qualifications), it follows that $L$ satisfies the strong max-min property,

$$\max_{\lambda\ge0}\max_\nu\min_{x\in X}L(x,\lambda,\nu)
= \min_{x\in X}\max_{\lambda\ge0}\max_\nu L(x,\lambda,\nu)$$

Provided $L$ (therefore $f$) satisfies certain regularity properties (smoothness and compact domain), this equality implies the existence of a saddle-point $(x^\ast, \lambda^\ast, \nu^\ast)$ satisfying $x^\ast \in \Omega$, $\lambda\ge0$ and,

$$L(x^\ast,\lambda,\nu) \le L(x^\ast,\lambda^\ast,\nu^\ast)
\le L(x,\lambda^\ast,\nu^\ast)$$

for all $x\in X,\lambda\ge0,\nu$. The saddle-point also gives the minimax value of $L$, therefore,

$$f^\ast = L(x^\ast,\lambda^\ast,\nu^\ast)$$

## Stationarity conditions

If $x^\ast$ lies in the interior of $X$, then $L$ must be critical w.r.t. to $x$,

$$\frac{\partial}{\partial x_j} L(x^\ast,\lambda^\ast,\nu^\ast)
= \frac{\partial}{\partial x_j} f(x^\ast)
+\sum_i\lambda_i^\ast \frac{\partial}{\partial x_j}g(x^\ast)
+\sum_j\nu_j\frac{\partial}{\partial x_j}h_j^\ast(x^\ast) = 0$$

If $\lambda_i^\ast*>0$, then $L$ is critical w.r.t to $\lambda_i$ also, which implies that $g_i(x^\ast)=0$. Since $\nu$ is not constrained, we have

$$\frac{\partial}{\partial\nu_j} L(x^\ast,\lambda^\ast,\nu^\ast) = h_j(x^\ast) = 0$$

for all $j$, which simply reproduces the equality constrains.

## References

1. Boyd, Stephen P., and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004. Specially Chapter 5 on Lagrangian duality.
2. [The minimax inequality and saddle-points]({% post_url 2018-04-24-minimax %})